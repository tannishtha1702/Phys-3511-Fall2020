{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2b Due: 9/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit this assignment as Assignment2b_FirstName_LastName\n",
    "\n",
    "In this assignment you will explore fitting data and assessing how well your fit describes the different data sets.\n",
    "\n",
    "Assignment Overview:\n",
    "* Fit data and use $\\chi^2$ and the $\\chi^2$ test to assess \n",
    "* Analyze the efficiency of your data provided differnt threshold levels using your fit results \n",
    "\n",
    "For this assingment you can make use of the numpy, matplotlib, and the scipy packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify imports here\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import integrate\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.special as sp\n",
    "\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: W Boson Mass\n",
    "\n",
    "Finding the *true* values of a quantity relies on analyzing results for many experiments. One quantity that has been measured many times is the W boson mass see Wikipedia https://en.wikipedia.org/wiki/W_and_Z_bosons and the particle data group (PDG) https://pdg.lbl.gov/2018/listings/rpp2018-list-w-boson.pdf \n",
    "\n",
    "**a)** In this problem you will analyze measurements of the W boson from various experiments and determine if the values are consistnet and given this data set, what the best fit value is. Start by reading in the data file Wmass_data.txt, which contains an experiment number, W mass in units of $GeV/c^2$ and its uncertainty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = open (\"/Users/tannishtha_nandi/Desktop/Wmass_data.txt\")\n",
    "N, wmass, uw = np.loadtxt (\"/Users/tannishtha_nandi/Desktop/Wmass_data.txt\", unpack = True)\n",
    "print (N)\n",
    "print (wmass)\n",
    "print (uw)\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([0.15,0.1,0.8,0.8])\n",
    "axes.errorbar(N, wmass, yerr = uw, fmt='.')\n",
    "axes.set_xlabel('Experiment Number')\n",
    "axes.set_ylabel('W mass');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Compute the error weighted mean of the W mass and its uncertainty. How does the weighted mean compare to the bold faced average of the PDG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmean = np.sum(wmass/uw**2)/np.sum(1.0/uw**2)\n",
    "print('Weighted Mean',wmean)\n",
    "wsduw = ((np.average((wmass-wmean)**2, weights=uw))/(len(N)-1))**0.5\n",
    "print('Weighted Standard Deviation of Uncertainty',wsduw)\n",
    "wsum = 1/ (wsduw)**2\n",
    "print(\"Weighted sum\",wsum)\n",
    "\n",
    "def func (s, t):\n",
    "    return t\n",
    "            \n",
    "popt, pcov = curve_fit(func ,N, wmass, sigma = uw, absolute_sigma = True)\n",
    "print ('Popt', popt)\n",
    "print ('Covariance',pcov)\n",
    "\n",
    "print (\"The weighted mean and the Bold Faced average of the PDG are equal to each other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Calculate the $\\chi^2$, degrees of freedom, reduced $\\chi^2$, and p-value. The p-value can be calculated using *gammaincc(dof / 2.0, chisq / 2.0)* from *scipy.special*. Based on the p-value are the data consistant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chisq = np.sum ((wmass-func(N,*popt))**2/uw**2)\n",
    "dof = len (N) - len(popt)-1\n",
    "rchisq = chisq/dof\n",
    "pvalue = sp.gammaincc(dof/2.0, chisq/2.0)\n",
    "\n",
    "print (\"Chi Square\",chisq)\n",
    "print (\"Degree of Freedom\",dof)\n",
    "print (\"Reduced Chi Square\",rchisq)\n",
    "print (\"P value\",pvalue)\n",
    "\n",
    "#print(\"The data seems consistent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Plot the measurement number vs. the W mass. Don't forget to include the error bars on the W mass measurements. Then Fit a line of the form $y = p_0$, where $p_0$ is a constant parameter.\n",
    "\n",
    "How does your $p_0$ value compare to the weighted mean you calculated earlier in part b)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_axes([0.15,0.1,0.8,0.8])\n",
    "axes.errorbar(wmass, N, xerr = uw, fmt='.')\n",
    "axes.set_ylabel('Experiment Number')\n",
    "axes.set_xlabel('W mass');\n",
    "plt.axvline ( x = popt, color= 'g');\n",
    "print(\"The po value is same as the weighted mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Proton Charge Radius\n",
    "\n",
    "We will carry an identical analysis as we did in Problem 1, but on a different quantity, the proton charge radius. The proton charge radius has been a recent hot topic in the nuclear physics field, as new designed experiments using muonic hydorgen have made very percise measurements of it. See https://www.nature.com/articles/s41586-019-1721-2\n",
    "\n",
    "**a)** Import the data set proton_radius_data.txt, which includes the experiment number, the proton charge radius, and its uncertainty measured in $fm$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = open (\"/Users/tannishtha_nandi/Desktop/proton_radius_data.txt\")\n",
    "N, pcr, up = np.loadtxt (\"/Users/tannishtha_nandi/Desktop/proton_radius_data.txt\", unpack = True)\n",
    "print (N)\n",
    "print (pcr)\n",
    "print (up)\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "axes.errorbar(N, pcr, yerr = up , fmt='.')\n",
    "axes.set_xlabel('Experiment Number')\n",
    "axes.set_ylabel('Proton Charge Radius');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Compute the error weighted mean of the proton charge radius and its uncertainty. \n",
    "\n",
    "You can also compare this to the PDG value (pgs. 6 and 7): https://pdg.lbl.gov/2018/listings/rpp2018-list-p.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmean = np.sum(pcr/up**2)/np.sum(1.0/up**2)\n",
    "print('Weighted Mean',pmean)\n",
    "wsdup = ((np.average((pcr-pmean)**2, weights=up))/(len(N)-1))**0.5\n",
    "print('Weighted Standard Deviation of Uncertainty',wsdup)\n",
    "\n",
    "def f (x, y):\n",
    "    return y\n",
    "            \n",
    "popt, pcov = curve_fit(f ,N, pcr, sigma = up, absolute_sigma = True)\n",
    "print ('Popt', popt)\n",
    "print ('Covariance',pcov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Calculate the  $\\chi^2$, degrees of freedom, reduced $\\chi^2$ and p-value. Based on the p-value are the data consistant? Do you see what all of the fuss is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq = np.sum ((pcr-func(N, *popt))**2/up**2)\n",
    "dof = len (N) - len(popt)-1\n",
    "rchisq = chisq/dof\n",
    "pvalue = sp.gammaincc(dof/2.0, chisq/2.0)\n",
    "\n",
    "print (\"Chi Square\",chisq)\n",
    "print (\"Degree of Freedom\",dof)\n",
    "print (\"Reduced Chi Square\",rchisq)\n",
    "print (\"P value\",pvalue)\n",
    "\n",
    "#print(\"The data seems consistent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Plot the measurement number vs. the proton charge radius. Don't forget to include the error bars on the proton charge radius measurements. Then Fit a line of the form  $y = p_0$ , where $p_0$ is a constant parameter.\n",
    "\n",
    "How does your $p_0$ value compare to the weighted mean you calculated earlier in part b)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_axes([0.15,0.1,0.8,0.8])\n",
    "axes.errorbar(pcr, N, xerr = up, fmt='.')\n",
    "axes.set_ylabel('Experiment Number')\n",
    "axes.set_xlabel('Proton Charge Radius');\n",
    "plt.axvline ( x = popt, color= 'g');\n",
    "print(\"The po value is same as the weighted mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Selecting Data\n",
    "\n",
    "In particle physics we sometimes want to measure a particlular particle that is created from the many resulting from a collision in a particle collider. In recording these collision events we typically measure other particles which are not the ones we are intersted in. The events we are interested in we reffer to as our signal, whereas the ones we are not interested in we reffer to as a background. \n",
    "\n",
    "**a)** The provided data set (Ep_data.txt) contains values of particle energy/momentum (E/p), the number of particles, and the uncertainty on the number of particles. Import the data and plot the number of particles vs. E/p and be sure to include the error bars on the particle counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = open (\"/Users/tannishtha_nandi/Desktop/Ep_data.txt\")\n",
    "r, N, ue = np.loadtxt (\"/Users/tannishtha_nandi/Desktop/Ep_data.txt\", unpack = True)\n",
    "print (r)\n",
    "print (N)\n",
    "print (ue)\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([0.15,0.1,0.8,0.8])\n",
    "axes.errorbar(r, N, yerr = ue, fmt='.')\n",
    "axes.set_ylabel('Number of Particles')\n",
    "axes.set_xlabel('Particle energy/ momentum');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** You should notice that there appear to be two clear distributions here. One which seems to be centered E/p = 0.6 and another around E/p = 1. The population at the lower E/p represent pions, whereas the population around E/p = 1 are electrons. For this exersice we will treat the pions as a background and the electrons as our signal. We will model each particle type to have a Gaussian distribution. Define two functions, one that returns a value computed from a Gaussian value, and anther function that returns a value computed from the sum of two Gaussian functions. Then make a fit to the data using the sum of two Gaussian functions. Each of your Gaussian functions can take the form of:\n",
    "\n",
    "$G_1(x) = p_1 e^{-(x-p_2)^2/(2p_3)}$\n",
    "\n",
    "where the $p_1, p_2,$ and $p_3$ are three parameters for the one Gaussian function. You will have 3 more different parameters for the other Gaussian function $G_2(x)$. \n",
    "\n",
    "Note: Did you get a negative value for the gaussian widths from your fit? We know that a negative value is not physical. Try to give some initial parameters for the fit to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pions = r<1\n",
    "electrons = r>1\n",
    "\n",
    "def electron(x,a,b,c):\n",
    "    return a*np.exp(-((x-b)**2)/(2*c))\n",
    "def pion(x,d,e,f):\n",
    "    return d*np.exp(-((x-e)**2)/(2*f))\n",
    "def gauc (x,a,b,c,d,e,f):\n",
    "    return ((a*np.exp(-((x-b)**2)/(2*c)))+(d*np.exp(-((x-e)**2)/(2*f))))\n",
    "\n",
    "popt, pcov = curve_fit(gauc, r,N, sigma = ue, absolute_sigma = True)\n",
    "print ('Popt', popt)\n",
    "print ('Covariance',pcov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Calculate your $\\chi^2$, degrees of freedom, reduced $\\chi^2$, and p-value for the fit to the data.\n",
    "Based on those statistics above is this a good fit? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** On the same graph, plot your data, the total fit to it, and the single Gaussian functions computed using the parameter results from your 2 Gaussian function fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r)\n",
    "chisq = np.sum ((N-gauc(r, *popt))**2/ue**2)\n",
    "dof = len (N) - len(popt)-1\n",
    "rchisq = chisq/dof\n",
    "pvalue = sp.gammaincc(dof/2.0, chisq/2.0)\n",
    "\n",
    "print (\"Chi sqaure\",chisq)\n",
    "print (\"Degree of freedome\",dof)\n",
    "print (\"Reduced Chi square\",rchisq)\n",
    "print (\"P value\", pvalue)\n",
    "a= popt[0]\n",
    "b= popt[1]\n",
    "c=popt[2]\n",
    "d=popt[3]\n",
    "e=popt[4]\n",
    "f=popt[5]\n",
    "x=r\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([0.15,0.1,0.8,0.8])\n",
    "axes.errorbar(r, N, yerr = ue, fmt='.', label = 'data')\n",
    "axes.set_ylabel('Number of Particles')\n",
    "axes.set_xlabel('Particle energy/ momentum');\n",
    "axes.plot(r,gauc(x,a,b,c,d,e,f),'r--',label= 'Sum of Gaussian Functions Fit')\n",
    "axes.plot(r,electron(x,a,b,c),'b--', label = \"Electron Fit\")\n",
    "axes.plot(r,pion(x,d,e,f),'g--', label= 'Pion Fit')\n",
    "axes.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** We can use the $E/p$ distribution to try to select the maximum number of electron while minimizing the pions that *leak* into our electron signal. We can do this by requireing our selected sample to be larger than some $E/p$ threshold value. Any data that has an $E/p$ value lower then the threshold we throw it out. In a physics analysis this is called a cut. However we need to be careful, if we place a cut at $E/p$ that is too large we will have a really clean electron sample, but throw away a lot of good electrons. On the other hand if we make the $E/p$ cut too low we will keep most of our electrons, but let in a lot of background (pions). So we must compormise between clean data and statistics. To do this lets calculat the total number of electrons we have from $0.0 < E/p < 2$. This can be obtained by integrating (you can use scipy integrators, I used *integrate.quad* when doing this exersise)the electron contribution from our fit. We will call this number e_tot. Do a similar thing for the total pions and call that number pi_tot. \n",
    "\n",
    "For 10 equally spaced E/p thresholds between 0.3 and 0.8, calculate the number of electrons that are above each of the thresholds, we can call this array e_sig and can be obtained by integrating from the E/p threshold to the E/p = 2. Do a similar thing for the pion distribution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitot= r<1\n",
    "etot = r>1\n",
    "N= 10\n",
    "th=np.zeros(N)\n",
    "limits = np.linspace(0.3,0.8,N)\n",
    "print(\"Threshold Limits\",limits)\n",
    "\n",
    "e_tot = np.array(integrate.quad(electron, 0,2, args = (popt[0],popt[1],popt[2])))\n",
    "print (\"e_tot\",e_tot)\n",
    "\n",
    "pi_tot = np.array(integrate.quad(pion, 0,2, args = (popt[3],popt[4],popt[5])))\n",
    "print (\"pi_tot\",pi_tot)\n",
    "\n",
    "\n",
    "e_sig = np.zeros ((len(limits),2))\n",
    "pi_sig = np.zeros ((len(limits),2))\n",
    "\n",
    "\n",
    "for i in range (len(limits)):\n",
    "    e_sig[i] = integrate.quad(electron, limits[i] ,2, args = (popt[0],popt[1],popt[2])) \n",
    "print (\"e_sig\", e_sig)\n",
    "\n",
    "for i in range (len(limits)):\n",
    "    pi_sig[i] = integrate.quad(pion, limits[i] ,2, args = (popt[3],popt[4],popt[5])) \n",
    "print (\"pi_sig\", pi_sig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Plot the ratios e_sig/e_tot and pi_sig/pi_tot as a function of E/p threshold on the same graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "er= e_sig/e_tot\n",
    "pr= pi_sig/pi_tot\n",
    "print('Ratio of Electrons',er)\n",
    "print('Ratio of Pions',pr)\n",
    "\n",
    "erpercent = er*100\n",
    "prpercent = pr*100\n",
    "print('Percentage of Electrons', erpercent)\n",
    "print('Percentage of Pions', prpercent)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([0.15,0.1,0.8,0.8])\n",
    "axes.set_ylabel('Percentage of the Ratio')\n",
    "axes.set_xlabel('Threshold Limits');\n",
    "plt.scatter(limits,erpercent[:,0],label= 'Electrons')\n",
    "plt.scatter(limits,prpercent[:,0],label = \"Pions\")\n",
    "axes.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g)** When the e_sig/etot ratio is 90%, how what percentage of the pion distribution is contaminating our electron sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Electron Ratio :90.0684%\", \"Pions Contamination: 19.1286%\") #From graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
